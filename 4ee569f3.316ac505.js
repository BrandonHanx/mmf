(window.webpackJsonp=window.webpackJsonp||[]).push([[19],{110:function(e,t,n){"use strict";n.d(t,"a",(function(){return p})),n.d(t,"b",(function(){return b}));var a=n(0),r=n.n(a);function c(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){c(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},c=Object.keys(e);for(a=0;a<c.length;a++)n=c[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var c=Object.getOwnPropertySymbols(e);for(a=0;a<c.length;a++)n=c[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=r.a.createContext({}),u=function(e){var t=r.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=u(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},d=r.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,c=e.originalType,i=e.parentName,l=o(e,["components","mdxType","originalType","parentName"]),p=u(n),d=a,b=p["".concat(i,".").concat(d)]||p[d]||m[d]||c;return n?r.a.createElement(b,s(s({ref:t},l),{},{components:n})):r.a.createElement(b,s({ref:t},l))}));function b(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var c=n.length,i=new Array(c);i[0]=d;var s={};for(var o in t)hasOwnProperty.call(t,o)&&(s[o]=t[o]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var l=2;l<c;l++)i[l]=n[l];return r.a.createElement.apply(null,i)}return r.a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},89:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return s})),n.d(t,"toc",(function(){return o})),n.d(t,"default",(function(){return u}));var a=n(3),r=n(8),c=(n(0),n(110)),i={id:"metrics",title:"Adding a custom metric",sidebar_label:"Adding a custom metric"},s={unversionedId:"tutorials/metrics",id:"tutorials/metrics",isDocsHomePage:!1,title:"Adding a custom metric",description:"Custom Metrics",source:"@site/docs/tutorials/metrics.md",slug:"/tutorials/metrics",permalink:"/docs/tutorials/metrics",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/tutorials/metrics.md",version:"current",lastUpdatedBy:"ryan-qiyu-jiang",lastUpdatedAt:1631715611,sidebar_label:"Adding a custom metric",sidebar:"docs",previous:{title:"Adding a dataset",permalink:"/docs/tutorials/dataset"},next:{title:"Tutorial: Adding a model - Concat BERT",permalink:"/docs/tutorials/concat_bert_tutorial"}},o=[],l={toc:o};function u(e){var t=e.components,n=Object(r.a)(e,["components"]);return Object(c.b)("wrapper",Object(a.a)({},l,n,{components:t,mdxType:"MDXLayout"}),Object(c.b)("h1",{id:"custom-metrics"},"Custom Metrics"),Object(c.b)("p",null,"This is a tutorial on how to add a new metric to MMF."),Object(c.b)("p",null,"MMF is agnostic to the kind of metrics that can be added to it.\nAdding a metric requires adding a metric class and adding your new metric to your config yaml.\nFor example, the ",Object(c.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/mmf/blob/master/website/docs/tutorials/concat_bert_tutorial.md"},"ConcatBERT")," model uses the ",Object(c.b)("inlineCode",{parentName:"p"},"binary_f1")," metric when evaluating on the hateful memes dataset.\nThe metric class is ",Object(c.b)("inlineCode",{parentName:"p"},"BinaryF1")," defined in ",Object(c.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/mmf/blob/master/mmf/modules/metrics.py"},"mmf/modules/metrics.py"),"\nThe metric key ",Object(c.b)("inlineCode",{parentName:"p"},"binary_f1")," is added to the list of metrics in the config yaml at ",Object(c.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/mmf/blob/15fa63071bfaed56db43deba871cfec76439c66f/projects/others/concat_bert/hateful_memes/defaults.yaml#L28"},"mmf/projects/hateful_memes/configs/concat_bert/defaults.yaml"),"."),Object(c.b)("h1",{id:"metric-class"},"Metric Class"),Object(c.b)("p",null,"Add your metric class to metrics.py. It should be a subclass of ",Object(c.b)("inlineCode",{parentName:"p"},"BaseMetric"),".\nMetrics should implement a function calculate with signature ",Object(c.b)("inlineCode",{parentName:"p"},"calculate(self, sample_list, model_output, *args, **kwargs)"),",\nwhere sample_list (",Object(c.b)("inlineCode",{parentName:"p"},"SampleList"),") is the current batch and model_output is a dict return by your model for current sample_list."),Object(c.b)("pre",null,Object(c.b)("code",{parentName:"pre",className:"language-python"},'@registry.register_metric("f1")\nclass F1(BaseMetric):\n    """Metric for calculating F1. Can be used with type and params\n    argument for customization. params will be directly passed to sklearn\n    f1 function.\n    **Key:** ``f1``\n    """\n\n    def __init__(self, *args, **kwargs):\n        super().__init__("f1")\n        self._multilabel = kwargs.pop("multilabel", False)\n        self._sk_kwargs = kwargs\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """Calculate f1 and return it back.\n\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n\n        Returns:\n            torch.FloatTensor: f1.\n        """\n        scores = model_output["scores"]\n        expected = sample_list["targets"]\n\n        if self._multilabel:\n            output = torch.sigmoid(scores)\n            output = torch.round(output)\n            expected = _convert_to_one_hot(expected, output)\n        else:\n            # Multiclass, or binary case\n            output = scores.argmax(dim=-1)\n            if expected.dim() != 1:\n                # Probably one-hot, convert back to class indices array\n                expected = expected.argmax(dim=-1)\n\n        value = f1_score(expected.cpu(), output.cpu(), **self._sk_kwargs)\n\n        return expected.new_tensor(value, dtype=torch.float)\n\n@registry.register_metric("binary_f1")\nclass BinaryF1(F1):\n    """Metric for calculating Binary F1.\n\n    **Key:** ``binary_f1``\n    """\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average="micro", labels=[1], **kwargs)\n        self.name = "binary_f1"\n')),Object(c.b)("h1",{id:"metrics-config"},"Metrics Config"),Object(c.b)("p",null,"Add the name of your new metric class to your evaluation config. Multiple metrics can be specified using a yaml array."),Object(c.b)("pre",null,Object(c.b)("code",{parentName:"pre",className:"language-yaml"},"evaluation:\n  metrics:\n  - accuracy\n  - roc_auc\n  - binary_f1\n")),Object(c.b)("p",null,"For metrics that take parameters your yaml config will specify params. You can also specify a custom key to be assigned to the metric. For ",Object(c.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/mmf/blob/master/projects/unit/configs/vg/single_task.yaml"},"example"),","),Object(c.b)("pre",null,Object(c.b)("code",{parentName:"pre",className:"language-yaml"},"evaluation:\n  metrics:\n  - type: detection_mean_ap\n    key: detection_mean_ap\n    datasets:\n    - detection_visual_genome\n    params:\n      dataset_json_files:\n        detection_visual_genome:\n          val: ${env.data_dir}/datasets/visual_genome/detection_split_by_coco_2017/annotations/instances_val_split_by_coco_2017.json\n\n")),Object(c.b)("p",null,"If your model uses early stopping, make sure that the early_stop.criteria is added as an evaluation metric. For example the ",Object(c.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/mmf/blob/master/projects/ban/configs/vizwiz/defaults.yaml"},"vizwiz")," config,"),Object(c.b)("pre",null,Object(c.b)("code",{parentName:"pre",className:"language-yaml"},"evaluation:\n  metrics:\n  - vqa_accuracy\n\ntraining:\n  early_stop:\n    criteria: vizwiz/vqa_accuracy\n    minimize: false\n")),Object(c.b)("h1",{id:"multi-metric-classes"},"Multi-Metric Classes"),Object(c.b)("p",null,"If a loss class is responsible for calculating multiple metrics, for example, maybe due to shared calculations, you can return a dictionary of tensors."),Object(c.b)("p",null,"For an example, take a look at the ",Object(c.b)("inlineCode",{parentName:"p"},"BinaryF1PrecisionRecall")," class in ",Object(c.b)("a",{parentName:"p",href:"https://github.com/facebookresearch/mmf/blob/master/mmf/modules/metrics.py"},"mmf/modules/metrics.py")),Object(c.b)("pre",null,Object(c.b)("code",{parentName:"pre",className:"language-python"},'@registry.register_metric("f1_precision_recall")\nclass F1PrecisionRecall(BaseMetric):\n    """Metric for calculating F1 precision and recall.\n    params will be directly passed to sklearn\n    precision_recall_fscore_support function.\n    **Key:** ``f1_precision_recall``\n    """\n\n    def __init__(self, *args, **kwargs):\n        super().__init__("f1_precision_recall")\n        self._multilabel = kwargs.pop("multilabel", False)\n        self._sk_kwargs = kwargs\n\n    def calculate(self, sample_list, model_output, *args, **kwargs):\n        """Calculate f1_precision_recall and return it back as a dict.\n        Args:\n            sample_list (SampleList): SampleList provided by DataLoader for\n                                current iteration\n            model_output (Dict): Dict returned by model.\n        Returns:\n            Dict(\n                \'f1\':         torch.FloatTensor,\n                \'precision\':  torch.FloatTensor,\n                \'recall\':     torch.FloatTensor\n            )\n        """\n        scores = model_output["scores"]\n        expected = sample_list["targets"]\n\n        if self._multilabel:\n            output = torch.sigmoid(scores)\n            output = torch.round(output)\n            expected = _convert_to_one_hot(expected, output)\n        else:\n            # Multiclass, or binary case\n            output = scores.argmax(dim=-1)\n            if expected.dim() != 1:\n                # Probably one-hot, convert back to class indices array\n                expected = expected.argmax(dim=-1)\n\n        value_tuple = precision_recall_fscore_support(\n            expected.cpu(), output.cpu(), **self._sk_kwargs\n        )\n        value = {\n            "precision": expected.new_tensor(value_tuple[0], dtype=torch.float),\n            "recall": expected.new_tensor(value_tuple[1], dtype=torch.float),\n            "f1": expected.new_tensor(value_tuple[2], dtype=torch.float),\n        }\n        return value\n\n\n@registry.register_metric("binary_f1_precision_recall")\nclass BinaryF1PrecisionRecall(F1PrecisionRecall):\n    """Metric for calculating Binary F1 Precision and Recall.\n    **Key:** ``binary_f1_precision_recall``\n    """\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(average="micro", labels=[1], **kwargs)\n        self.name = "binary_f1_precision_recall"\n')))}u.isMDXComponent=!0}}]);